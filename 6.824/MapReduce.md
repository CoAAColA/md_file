# MapReduce

## Abstract

MapReduce是一个用于处理和生成大数据集的编程模型和相关实现。

其中`map`方法处理一个键值对来生产一个中间键值对集合，`reduce`方法将同一中间键对应的所有中间值合并起来。

用这个方法风格编写的程序是自动并行化的

运行时系统负责划分输入数据，在一组机器上调度程序的执行，处理机器故障以及管理所需的机器间通信的细节

## 1 Introduction

输入数据通常很大，为了在合理的时间内完成计算，计算往往分布在成百上千台机器上。如何并行计算、分配数据、处理故障、负载均衡共同导致了大量的复杂配套代码来处理这些问题，从而模糊了原来的简单计算。因此，我们设计了一个新的抽象来允许我们专注于简单的计算而掩盖了上述导致混乱的问题。

新的抽象的灵感来源于`Lisp`等函数式编程语言中的`map`和`reduce`原语。我们意识到，我们绝大多数的计算都涉及对输入中的每个逻辑记录执行一个`map`操作来生产一个中间键值对集合，然后对共享同一键的所有值应用`reduce`操作，以适当地组合衍生的数据。

我们使用这个函数模型允许我们可以轻松地并行化大型计算，并将重新执行作为容错的主要机制。

## 2 Programming Model

计算采用一组输入键值对，生成一组输出键值对。

`map`函数由用户编写，获取一个输入对，生成一组中间键值对，`MapReduce`库将同一键对应的所有中间值分组然后传递给`reduce`

`reduce`同样由用户编写，接受一个中间键以及对应的一组值。它将这些值合并在一起，形成一组可能较小的值。通常，每次`reduce`调用只生成0个或者1个输出值。

中间值通过迭代器传递给用户的`reduce`方法，这允许我们处理一组太大以至于无法放进内存的一组值。

## 3 Implementation

### 3.1 Execution Overview

`map`调用通过自动将输入数据划分为`M`个切片而分布在多台机器上。输入的切分可以被不同的机器并行处理。

`reduce`调用通过用分区函数将中间键空间划分到`R`个块里来分发的。用户指定`R`的值和分区函数。

![image-20230911163226790](/home/liam/.config/Typora/typora-user-images/image-20230911163226790.png)

图1展示了`MapReduce`总体的流程，当用户程序调用`MapReduce`方法，会发生下列一系列动作（动作编号与图1中的编号对应）：

1. 用户程序中的`MapReduce`库首先将输入文件切分成`M`块，每块一般16MB到64MB大小（用户可以指定可选参数来控制块的大小），然后它会在机器集群上启动当前程序的许多份复制

2. 多份程序的复制中有一份是特殊的——也就是`master`，其余的复制都是`workers`——由`master`来分配任务。一共有`M`个`map`任务以及`R`个`reduce`任务来分配。`master`选择空闲的`workers`然后给每一个分配一个`map`任务或者`reduce`任务。

3. 一个分配了`map`任务的`worker`从对应的输入划分中读取内容。然后将对输入划分进行语法分析得到的键值对传递给用户定义的`map`方法。`map`方法生成的中间键值对缓存在内存中。

4. 周期性地，缓存的键值对被写入本地磁盘中，并通过分区函数划分为`R`个区域。这些缓冲对在本地磁盘上的位置被传递回`master`，`master`负责将这些位置转发给`reduce workers`。

5. 当一个`reduce worker`收到`master`关于这些位置的通知时，它用远程处理调用（RPC）来读取`map workers`本地磁盘上的缓存数据。当一个`reduce worker`读取完了所有的中间数据，它就可以按照中间键对对中间数据排序，这样的话相同的键对应的键值对就会被分组在一起。需要排序是因为可能有很多不同的键映射到同一个`reduce`任务上。如果中间数据太大以至于不能放入内存中，就使用外部排序。

   > 这里将数据传递给reduce后，处理的流程是论文中提到的统计单词出现次数，如果是其他的处理流程，可能不需要接收全部数据后再排序这个步骤。论文前面也提到了`reduce`方法通过迭代器来接收中间值，允许我们处理不能放入内存中的数据量，也就是流式处理。

6. `reduce worker`对排序的中间数据进行迭代，对遇到的每一个唯一的中间键，它将键和对应的中间值的集合传递给用户的`reduce`方法。`reduce`方法的输出被追加到这个`reduce`分区的最终输出文件上。

7. 当所有的`map`任务和`reduce`任务完成后，`master`唤醒用户程序。此时，用户程序中的`MapReduce`调用返回到用户代码中。

成功完成后，`mapreduce`执行的输出在`R`个输出文件中可用（每个`reduce`任务一个，文件名由用户指定）。通常，用户不需要将这`R`个文件合并到一个文件，他们通常将这个`R`个文件作为另一个`MapReduce`调用的输入，或者从另一个能够处理输入被分区为多个文件的分布式应用程序中使用他们。

### 3.2 Master Data Structures

`master`持有一些数据结构。对每个`map`任务和每个`reduce`任务，它存储状态（`idle`，`in-progress`，`completed`），对于非空闲的任务，它还存储`worker`机器的id。

`master`是`map`任务传播中间文件区域的位置信息到`reduce`任务的管道。因此，对于每一个`completed map task`，`master`存储`map`任务生成的`R`个中间文件区域的位置和尺寸。`map`任务完成后，这个位置和尺寸信息的更新将被接收。这个信息被逐步推送到处于`in-progress`状态的`reduce`任务对应的`worker`上。

### 3.3 Fault Tolerance

因为`MapReduce`库被设计于用来帮助在成百上千台机器上处理大量数据，所以这个库必须能够优雅地容忍机器故障

#### Worker Failure

`master`定期ping一下每个`worker`。如果在一定时间内没有收到来自`worker`的回应，那么`master`将对应`worker`标记为故障。对应`worker`完成的任何`map`任务都会被设置为初始的`idle`状态，因此有资格在其他`worker`上进行调度。类似的，在一台故障的机器上的任何`map`任务或者`in-progress reduce`任务同样被设置为`idle`并可以被调度。

已完成的`map`任务会在出现故障时重新执行，因为他们的输出是存储在出现故障的机器的本地磁盘上，因此是无法访问的。已完成的`reduce`任务不需要被重新执行是因为它们的输出是存储在全球文件系统（GFS）上。

当一个`map`任务首先被`worker A`执行然后被`worker B`执行（因为 `A`故障了），所有执行`reduce`任务的`workers`都被通知到这一次重新执行。任何没有从`worker A`读取完成数据的`reduce`任务都将从`worker B`处读取数据。

#### Master Failure

让`master`周期性地写上述`master`数据结构的检查点很容易。如果`master`任务挂掉，一个新的副本可以从最后的检查点状态启动。然而，考虑到只有一个主机，它故障的可能性很小，因此如果`master`故障，我们当前的实现是终止`MapReduce`计算。用户可以检查这个条件，如果需要，可以重试`MapReduce`操作。

#### Semantics in the Presence of Failures

当用户提供的`map`和`reduce`操作符对于输入来说是确定性的时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。

我们依赖于`map`和`reduce`任务的输出的原子提交来实现这个属性。每个`in-progress`任务将它的输出写到私有的临时文件里。一个`reduce`任务产生一个这样的文件，一个`map`任务产生`R`个这样的文件。

当一个`map`任务完成的时候，`worker`发送一条包含`R`个临时文件名的信息给`master`。如果`master`收到一条来自已完成的`map`任务发送的完成信息，它会忽略这条信息。否则，它会在`master`数据结构中记录`R`个文件的文件名

当一个`reduce`任务完成的时候，`reduce worker`原子性地将其临时输出文件重命名为最终输出文件。如果同一个`reduce`任务在多台机器上执行，多个重命名调用将在同一个最终输出文件上执行。我们依赖于底层文件系统提供的原子性的重命名操作来保证最终文件系统状态只会包含这个`reduce`任务的一次执行结果所产生的数据。

> 同一个`reduce`任务在多台机器上执行的原因是否可能是因为该`worker`到`master`中的通信断开了，导致`master`调度一个新的`worker`来执行这个`reduce`任务，但老的`worker`仍在执行该`reduce`任务，也就导致了会有多个机器同时跑同一个`reduce`任务
>
> 可能是`Backup Tasks`导致的

我们绝大多数的`map`和`reduce`运算符都是确定性的，在这种情况下，我们的语义相当与顺序执行，这是的程序员很容易对程序的行为进行推理。

### 3.4 Locality

在我们的计算环境中，网络带宽是一种相对稀缺的资源。我们利用输入数据保存在组成我们集群的机器的本地磁盘的事实来保护网络带宽。GFS将每个文件分成64MB的块，并且在不同的机器上存储每个块的副本（一般是3个副本）。`MapReduce master`采取输入文件的位置信息来分配`map`任务到拥有输入文件的副本的机器上。如果做不到的话，它就尝试分配`map`任务到临近这个任务的输入数据的副本的机器上。

### 3.5 Task Granularity

我们将`map`阶段分成`M`块，将`reduce`阶段分成`R`块。`M`和`R`应该远大于`worker`的数量。每个`worker`执行多个不同的任务提升了动态负载均衡，同样加速了`worker`故障的恢复：故障机器完成的多个`map`任务可以分散给所有其他的`worker`

### 3.6 Backup Tasks

延长`MapReduce`操作所需总时间的一个常见原因是掉队者：一台机器需要异常长的时间来完成最后几个`map`或者`reduce`任务。掉队者的出现可以有多个原因：比如一台磁盘不好的机器可能经历频繁的可纠正错误导致他的读取性能从30MB/s降低到1MB/s。集群调度系统可能分配其他任务在这台机器上，因为对cpu，内存，本地磁盘，网络带宽的竞争导致它执行`MapReduce`代码更慢。

我们有一个通用的机制来减轻掉队者问题带来的影响，当一个`MapReduce`操作快完成时，`master`安排剩余的`in-progress`任务备份执行。当主执行或者备份执行完成后，任务都被标记为`completed`。我们调整了这个机制，使其通常只会将操作所用的计算资源增加百分之几，对于大型`MapReduce`操作来说这个操作尤其有用。

## 4 Refinements

### 4.3 Combiner Function

我们允许用户指定一个可选的`Combiner`方法，它会在数据通过网络发送前做部分合并

`combiner`方法会在每一台执行`map`任务的机器上执行。`combiner`和`reduce`方法是用同样的代码实现的，唯一的区别是`MapReduce`库如何处理方法的输出。`reduce`方法的输出被写到最终输出文件里。`combiner`方法的输出被写到将要被发送给`reduce`任务的中间文件内。

> `combiner`方法的作用应该是提前在本地做`reduce`的操作来减少数据规模，这样做的话可以减少网络IO，但前提是可以提前做，有的场景可能无法提前做`reduce`操作。

### 4.6 Skipping Bad Records

有时候代码中的bug或者第三方库的bug导致我们一处理特定的记录`MapReduce`操作就无法完成，这时我们可以跳过这些记录。我们提供了一种可选的执行模式，其中`MapReduce`库会检测哪些记录会导致确定性崩溃，然后就可以跳过这些记录来继续执行

### 4.9 Counters

`MapReduce`库提供了一个计数设施来统计不同的事件的发生次数。

来自不同`worker`机器的计数值被周期地发送到`master`（依靠ping响应），`master`将来自完成了的`map`和`reduce`任务的计数值聚集起来，当`MapReduce`操作完成后将其发送给用户代码。在聚集计数值时，`master`消除来自相同的`map`或者`reduce`任务的重复执行产生的影响来避免双重计数（重复执行可以来自于`backup tasks`或者故障导致的重执行）。

`MapReduce`库自动维护一些计数器，比如处理了多少输入键值对，产生了多少键值对。

计数器还可以用来检测`MapReduce`操作的健全性，比如在一些情况下，我们需要确保产生的键值对数量等于输入的键值对数量。











